{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The target table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercises, you've calculated a DataFrame called recommendations. It contains pairs of user_id's' and course_id's, with a rating that represents the average rating of this course. The assumption is the highest rated course, which is eligible for a user would be best to recommend.\n",
    "\n",
    "It's time to put this table into a database so that it can be used by several products like a recommendation engine or an emailing system.\n",
    "\n",
    "Since it's a pandas.DataFrame object, you can use the .to_sql() method. Of course, you'll have to connect to the database using the connection URI first. The recommendations table is available in your environment.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Fill in the connection URI for the Postgres database on host localhost with port 5432. You can connect with user repl and password password. The database name is dwh.\n",
    "Complete the load_to_dwh() function. It should write to the \"recommendations\" table and replace the table if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "def load_to_dwh(recommendations):\n",
    "    recommendations.to_sql(\"recommendations\", db_engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercises, you've completed the extract, transform and load phases separately. Now all of this is put together in one neat etl() function that you can discover in the console.\n",
    "\n",
    "The etl() function extracts raw course and ratings data from relevant databases, cleans corrupt data and fills in missing value, computes average rating per course and creates recommendations based on the decision rules for producing recommendations, and finally loads the recommendations into a database.\n",
    "\n",
    "As you might remember from the video, etl() accepts a single argument: db_engines. You can pass this to the task using op_kwargs in the PythonOperator. You can pass it a dictionary that will be filled in as kwargs in the callable.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Complete the DAG definition, so it runs daily. Make sure to use the cron notation.\n",
    "Complete the PythonOperator() by passing the correct arguments. Other than etl, db_engines is also available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DAG so it runs on a daily basis\n",
    "dag = DAG(dag_id=\"recommendations\",\n",
    "          schedule_interval=\"0 0 * * *\")\n",
    "\n",
    "# Make sure `etl()` is called in the operator. Pass the correct kwargs.\n",
    "task_recommendations = PythonOperator(\n",
    "    task_id=\"recommendations_task\",\n",
    "    python_callable=etl,\n",
    "    op_kwargs={\"db_engines\": db_engines},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercises, you've learned how to calculate a table with course recommendations on a daily basis. Now that this recommendations table is in the data warehouse, you could also quickly join it with other tables in order to produce important features for DataCamp students such as customized marketing emails, intelligent recommendations for students and other features.\n",
    "\n",
    "In this exercise, you will get a taste of how the newly created recommendations table could be utilized by creating a function recommendations_for_user() which automatically gets the top recommended courses based per user ID for a particular rating threshold.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Complete the query in the recommendations_for_user() function definition. It should join the courses table.\n",
    "Complete the read_sql() function in recommendations_for_user(). The params argument is incomplete: it's missing a threshold.\n",
    "Run the recommendations_for_user() function you defined in the last statements and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations_for_user(user_id, threshold=4.5):\n",
    "    # Join with the courses table\n",
    "    query = \"\"\"\n",
    "    SELECT title, rating FROM recommendations\n",
    "    INNER JOIN courses ON courses.course_id = recommendations.course_id\n",
    "    WHERE user_id=%(user_id)s AND rating>%(threshold)s\n",
    "    ORDER BY rating DESC\n",
    "    \"\"\"\n",
    "    # Add the threshold parameter\n",
    "    predictions_df = pd.read_sql(query, db_engine, params = {\"user_id\": user_id, \n",
    "                                                             \"threshold\": threshold})\n",
    "    return predictions_df.title.values\n",
    "\n",
    "# Try the function you created\n",
    "print(recommendations_for_user(12, 4.65))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
